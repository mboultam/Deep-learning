{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1hFiHTNCdf4O52A-5Q1WMRDks9U4ndM_V","timestamp":1642736194132}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ab6faQL644HQ"},"source":["**TP n°1**\n","Notions abordées:\n","\n","En partie I:\n","- Apprentissage d'un perceptron par descente de gradient stochastique.\n","- Perceptron à plusieurs couches\n","- Fonction de coût adaptée aux tâches de classification.\n","\n","En partie II:\n","- Autres briques de bases des réseaux profond: couches de convolutions et non-linéarités.\n","- Analyse d'un réseau profond standard  entraîné sur imagenet.\n","\n","En partie III:\n","- Entraînement d'un CNN sur MNIST. Comparaison avec un perceptron multicouches.\n","- Méthodes d'initialisation, méthodes de régularisation.\n","\n","En partie IV:\n","- Apprentissage sur une carte graphique\n","- Amélioration de la descente de gradient: SGD avec inertie (*momentum*) et diminution progressive du taux d'apprentissage (*scheduler*).\n","- Transfer learning: fine tuning et freezing.\n","\n","\n","\n","Durée : 4 h"]},{"cell_type":"markdown","metadata":{"id":"_Ef31xZdxgEZ"},"source":["**Partie I**\n","\n","Cette partie introduit les réseaux de neurones à travers une présentation du perceptron. C'est aussi l'occasion de vous familiariser avec les commandes pytorch. Pytorch est l'une des trois librairies les plus utilisées pour le deep learning avec Keras et TensorFlow.\n","\n","Le \"deep learning\", c'est, par définition, l'apprentissage de réseaux de neurones \"profonds\" par descente de gradient stochastique. Par profond, on entend: constitués d'un succession de \"couches\" de neurones.\n","\n","Sous pytorch, les briques de base qui permettent de construire ces couches sont codées dans le module torch.nn."]},{"cell_type":"code","metadata":{"id":"p--IEVAx2CCo"},"source":["from matplotlib import pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKOJQm_HhfvV"},"source":["**A.** Dans un premier temps, nous revenons sur le perceptron.\n","Pour présenter un apprentissage par descente de gradient stochastique, \n","nous allons aborder un problème de classification binaire simple. "]},{"cell_type":"code","metadata":{"id":"A85R0bUv20fs","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1666184664730,"user_tz":-120,"elapsed":1069,"user":{"displayName":"pierre lepetit","userId":"00153244657746066434"}},"outputId":"af3fc2dd-37fc-4193-e452-85d40e7d07e3"},"source":["#Les données à séparer:\n","n = 100\n","std = 0.5\n","\n","#échantillon 1:\n","mean0 = torch.tensor((-1.,-1.))\n","ech0 = mean0 + std*torch.randn(n,2) \n","\n","#échantillon 2:\n","mean1 = torch.tensor((1.,1.))\n","ech1 = mean1 + std*torch.randn(n,2) \n","\n","echs = [ech0, ech1]\n","\n","\n","#scatter plot:\n","fig, ax = plt.subplots()\n","ax.axis([-3, 3, -3, 3])\n","\n","plt.title('data')\n","colors = ['b','r']\n","labels = ['0','1']\n","\n","for i,ech in enumerate(echs):\n","    x,y = ech.numpy()[:,0], ech.numpy()[:,1]\n","    ax.scatter(x,y, color = colors[i] )\n","\n","plt.legend(labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f0fb311ced0>"]},"metadata":{},"execution_count":3},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3df4xlZXkH8O+zszPOzroU97Ja3dmdha5FkBgpI9W00h+okE0D1VYKmaWLtNkyQF0TjdpOorHtJhpT47YUkQTalbldikWKIfxQokZNRJglaGEBJeossyG6DEXYrOvOzj7949zD3Lnzvue855z3/LzfT3KzO3fuPec9l+U5733e5zxHVBVERNQcq8oeABER+cXATkTUMAzsREQNw8BORNQwDOxERA3DwE5E1DAM7NRXROQ/ROSfyh4HUZ4Y2IkMRORbIvLXZY+DKA0GdiKihmFgp0YTkXNF5FEReVlE/gvAcOf514jIPSJyWET+r/P30c7vdgN4J4AbROSIiNzQeX6PiDwrIi+JyH4ReWdpB0YUgYGdGktEhgD8D4DbAKwH8GUAf9b59SoA/w5gDMBmAL8CcAMAqOoUgO8AuF5VX62q13fe8wiAt3a29Z8Aviwiw8UcDZE7BnZqsrcDGATweVVdUNX/RhCcoarzqnqnqh5V1ZcB7AbwB1EbU9XpzvtOqOo/A3gVgDNzPgaixBjYqcneAOCQLu90NwsAIjIiIl8UkVkReQnAtwGcKiIDto2JyEdE5EkR+aWIvAjgNwCclucBEKXBwE5N9hyAjSIiXc9t7vz5YQSz7d9V1VMAXNB5PnztsrannXz6RwFcBuA1qnoqgF92vZ6oMhjYqcm+B+AEgA+KyKCIvA/A+Z3frUOQV39RRNYD+GTPe38O4Iyun9d1tnUYwGoR+QSAU/IcPFFaDOzUWKp6HMD7AFwF4AUAfwHgK51ffx7AGgDPA3gIwP09b98D4M87FTP/AuCBzmt+hCCdcwzAszkfAlEqwhttEBE1C2fsREQNkzmwi8iwiDwsIj8QkSdE5FM+BkZEROlkTsV0Kg7WquoRERkE8F0Au1T1IR8DJCKiZFZn3UCnRvhI58fBzoOJeyKikmQO7ADQuahjP4CtAP5NVb9veM1OADsBYO3atee96U1v8rFrIqK+sX///udVdUPc67xWxYjIqQDuAvC3qvq47XXj4+M6MzPjbb9ERP1ARPar6njc67xWxajqiwC+CeBin9slIiJ3PqpiNnRm6hCRNQDeDeCprNslIqJ0fOTYXw9gbyfPvgrAHap6j4ftEhFRCj6qYn4I4FwPYyEiytXCwgLm5uZw7NixsocSaXh4GKOjoxgcHEz1fi9VMUREdTA3N4d169Zhy5YtWN70szpUFfPz85ibm8Ppp5+eahtsKUBEfePYsWNotVqVDeoAICJotVqZvlUwsBNRX6lyUA9lHSMDOxFRwzCwExEV7P7778eZZ56JrVu34tOf/rT37TOwExEVaHFxEddddx3uu+8+HDhwAPv27cOBAwe87oOBnYjIot0GtmwBVq0K/my3s2/z4YcfxtatW3HGGWdgaGgIl19+Oe6+++7sG+7CwE5EZNBuAzt3ArOzgGrw586d2YP7oUOHsGnTpld+Hh0dxaFDhzKOdjkGdiIig6kp4OjR5c8dPRo8X3UM7EREBgcPJnve1caNG/Hss0v3QZ+bm8PGjRuzbbQHAzsRkcHmzcmed/W2t70NP/7xj/HTn/4Ux48fx+23345LLrkk20Z7MLATERns3g2MjCx/bmQkeD6L1atX44YbbsBFF12Es846C5dddhne/OY3Z9to7z68bo2IqCEmJoI/p6aC9MvmzUFQD5/PYtu2bdi2bVv2DVkwsBMRWUxM+AnkRWMqhoioYRjYiYgahoGdiKhhGNiJiBqGgZ2IqGEY2ImICnb11Vfjta99Lc4555xcts/ATkRUsKuuugr3339/bttnYCcissmjby+ACy64AOvXr/eyLRNeoEREZBL27Q1bPIZ9e4HKX7XEGTsRkUmN+/YysBMRmeTVt7cADOxERCZ59e0tAAM7EeW2SFhrefXtBXDFFVfgHe94B55++mmMjo7illtuybzNbpkXT0VkE4AvAXgdAAVws6ruybpdIipIjRcJc5Vj3959+/Zl3kYUHzP2EwA+rKpnA3g7gOtE5GwP2yWiItR4kTB3ExPAz34GnDwZ/FmTE13mwK6qz6nqo52/vwzgSQB+b+BHRMvZUidpUio1XiQkM6917CKyBcC5AL7vc7tE1MWUOrnySmD7dkAEUF163iWlsnlz8FrT8w2kqhCRsocRScP/hil5WzwVkVcDuBPAh1T1JcPvd4rIjIjMHD582NduifqPKXUSBoLegOCSUjEtEg4OAkeOxM/8a7boOjw8jPn5+cyBM0+qivn5eQwPD6fehvg4QBEZBHAPgAdU9XNxrx8fH9eZmZnM+yXqS6tWrQzgUUSCHHGUdntpkXD9euDll4Hjx5d+PzIC3Hzz8pl/7zcH2+sqZGFhAXNzczh27FjZQ4k0PDyM0dFRDA4OLnteRPar6njc+zMHdgm+0+wF8IKqfsjlPQzsRBmcdhowP+/++rGxYOHP1ZYt5tRM73ZcX0feuAZ2H6mY3wNwJYA/FpHHOo/8br9N1M/abeClFZlOOxFgW8L/HV0XU7noWlk+qmK+q6qiqm9R1bd2Hvf6GBwRdWm3gR07gIUF9/eoAnv3Jst9u15xWeMrM5uOV54S1UGYz15cTP7epDXprldc5nhlJmXDwE5UB6ZKmCSSpEcmJoIF0LGxIJUzNmZfEF2zZunvrVa+C6c1q8ApE/uxE9VBVGAeGQkCbNSCatL0yMREdIA2VcT86lfJ9pEE2x4kwhk7UVVEzUhtgXlgIJgl79mzMi0SyiM9UnQbArY9SISBnagKwhnp7Gyw4BnOSMPgbstn7927NLsO0ydAEPCBlWkUX+mMoitiWIGTjKoW/jjvvPOUqJGmp1XHxlRFgj+np93eNzamGoT05Y+xsezb7n7/yMjy7Y+MJN+O63h9Knp/FQVgRh1iLAM7kS9ZAqeIOXCJ+Bufz+Do8yRRxf1VlGtgZyqGyJcseeAiasJ9pjNMlTM7dgTHmkfVSpJKHfLTKyYpthSgRrL1cHHt1ZJ335U8WwDUsG9MHRXZUoCIgGyz7t4ZaasVlDBeeeXKfuunnRa8RiT4u+vMOM8Lili1Ui0u+RrfD+bYqZF85YFt25mcVB0aWpkjHxwM3uOyuJp1AdamiDUC4uIpUSl8BE7bIufAgPl5QLXVKndxsYiqHnIO7MyxE1VN0n7rUYpqoRuXY2cO3gvm2InqKuoq06R8X8Bju8Aprmol7xw8+8gs5zKt9/1gKob6lmsePEmOPerh8wKe6emV+x8a8lennzZV00c17mCOnahikgQgW5CbnFRdtcotqPsObq2WeT+tVvx743Lwps9mcDDYdlyg76OrUhnYiaomawAyBT/bTHhgIH1Qt51Uok4iacc+ORn83vbZuJyo+qgih4GdKCvfVRxZA5At+PVuN8tMPepbRZbArhoEcdtYbZ+Ny0nQ9k2CM3YGdqJl8sjbZp2xRwU/XyegNKWW4f7j9ht1/C4zdtNJcHo6SNn0vs41918zDOxEWeSRt816sigil+w6c06SKonbtoj5s3E5Vttn4pL3ryHXwM5yRyKTPPp/Z21klWdLgLBcUDX9NuLKF6NaLphaKgwNLX+d6Vht/z1eeMF93E3kEv19Pzhjp8rLMjvO8wrLPLZtyn2nfUStFyT9xuJyrH1UEaPqPmNnYCcySZs2yZJuyRK0s9SA+wrqLgHV94mpj2rYVRnYibJLE4TSziCzzGZbrZULiK7BzXXR0pZTr0JA7aMeNK6Bnb1iiHxK25M9Sa90U98Vk7g+Me02sH179Daitr17d5BTP3gwyJPv3s2+LzljrxiiMqxfn+z5UJLFWlPflSTbBIKgfvXV8dswCRcxJyaCE8fJk8GfWYM6+714w8BOVAVJbtLhWpkTdTKZmgKOH3fbDrDUgGxgYKn6xWfgDb+FzM4G33hmZ4Of0+6jz08SDOxEPtnK7Obno9+XpJTRx31Qk5RtjowEQXZwEFhcDJ6bnQU+8AF/AdNn90ffJ4ka8hLYReRWEfmFiDzuY3tEtWULuiLRgSVJjbvpJGASVcsddXJotVaO4447gIWF5a9bWAB27Yofhwuf1w3wNn1+qmIAXADgdwA87vJ6VsVQY0WVD6aprbZVfExP23ukuOzP1IIXWLrNXq+o/fiQpJoorgqmwU3BUHS5I4AtDOzUt7qDTZqLd2zbjCopjCpVdCk97G0BvHat/T1RgT3NPV17A3PcsYbvCT/HqGNt8EVLlQvsAHYCmAEws3nz5tw/AKJCuMyc0waWuADlsr+oOvgkdehRPeCTHFfUfqO+ncT1kem9t2pVauw9q1xg735wxk6N4Nq4Km1giUspxHVcjNpv0llt1D6SfBNJM5t2uYjK1PWxgRctuQZ2VsUQpeVST56m2VcorgQyrFCJYls0TLpYOTZm38eqVW4VJ+22+SIsIHjetg2XBdTez8p3jX3NMLATpRUXcMbGzIHl2muB1auDoL96dfCzSVwJZFSwjRtnkrr5cCy93RZDi4vx5YRhCWIU2zbiyjt9dbhsEpdpfdwDwD4AzwFYADAH4K+iXs9UDDVCmsXLyUnz68NbxJleH6ZcBgaWvy5tD3Pbe+PSRXH3W82aTkky1jBN1aA0iwuwCRhRzmyBtdWyBxtbXnxgwG37IyNBgO1uABbe8DlpM7AkeWiXk0h4wwzTNl07SNry9VEnuD7CwE5UhKSLdFFBrZfrLNdUFuh70dBlLK2W/VuA7xl7Q6pckmJgJ6qiJDP2JH3SXW8A0l2aGfXNIulYRkaibyrtMuP3VcHTYK6BnYunREWyLSCank/SE6Z3gbS3Cda11wa9Xbp71szPBx0e4ypa2u1gOzZh1Y+thcHBg8tbJgDBwnE3EWDHDnP1Sh63KWw6l+jv+8EZO/U113xxkjr5uAt00s72o7aV9orPpDNwzthfAaZiiGqq9+5ILmkQlwAb9bClZGzbGhhY+R7XXHjSXi7Msb+CgZ2ojkxBbGhoZbWLqdwvySJl72NoyBwo0wRhXzeg7j3BhSe5Pitx7MbATlRHUYE5TN+YAlvS9ItraiOPNIhLXTpn6UaugZ33PCWqEts9U0MjI+b2BLZ7piZhui+r6f6qtjEk0W4HrQ5mZ4P9dh/zyAiwZo355iRx93FtON7zlKiO4iphkvZ+ybrvJDcASSLs5TI2tvJEdvSo/Y5TrIRxwsBOVCUud0dK0vult6zQ9tzQkL3fSp4NtZIGah+3BewDDOxEVdJb721iCm62hmHXXLN8tj09Ddx2W3D7u1CrBdx6azkdEG2ButVyvwcsreSSiPf94OIpkQOXXjGmqhjflSN59jZPc+ONPgZWxVQf/91SrN5/JJOTxVaLFFGdwv8RnLkGdlbFlCSvYgNqOFv1S2+1SFh1cvBgkO7YvTvdPyzX/VEhXKtiGNhLwv9fKBVbOWR3qeK11wI33bSyhDDNrMFlf1QYljtWHPsa9YneZlwut5CL2patGVe4CNlurwzqgL1MMk7SOy3lxffn6GtbVeWSr/H9YI49WQsOqimf+WnXZlxRV64muel0HseQVt6fY42uaAUXT6stSdM8qimfl+O7zgSiGoalbQNQ9uJmEZ9jTTpFugZ25thL1G4HLahNN5tnrr0BfOanXbdlW7wRCerX67gyX8bnWFHMsdfAxIT93xJz7Q3gMz/tui3ThUoiwYVKZQb1LHntMj7HmmNgL1mf/DvrT7arQdNcPem6LVNvl9tuA268Mfk+fQlre2dng9ny7Gzws2twL+NzrDuXfI3vB3PsS2q+lkNxfOany851p+Ujr53k2ONeW9fPUZljrxVf15IQVVKRee2GX/nHC5SIqBqKvBqv4Vf+cfGUiKqhyLw2r/wDwMBORHnL62YdJqxGAMDATkRFyPNmHd36peolhpfALiIXi8jTIvKMiHzcxzaJiBIr8ttBhWVePBWRAQA/AvBuAHMAHgFwhaoesL2Hi6dERMkVuXh6PoBnVPUnqnocwO0ALvWwXSIiSsFHYN8I4Nmun+c6zy0jIjtFZEZEZg4fPuxht0QF64d2r9QIhS2equrNqjququMbNmwoardEfmS9LJ6oQD4C+yEAm7p+Hu08R9QcU1PLr2YE0t+8gihnPgL7IwDeKCKni8gQgMsBfNXDdomqgxe+UI1kDuyqegLA9QAeAPAkgDtU9Yms26VqYFq5gxe+UI14ybGr6r2q+tuq+luq2l9XAjRY2rRyI08GvPCFaoRXnpLVrl3J08p1W2N0PgnxwheqEXZ3JKN2G9i+3fy7qG6rdWqu1/AOr9RAbNtLmdgCNAC0WsDzz5t/V6dbStbpJEQEsG0vZZS22KNOa4wsdKGmYmBvgDwWK6MC8Qsv2H9nu5fy7Gz1FlLrdBIiSoKBvebyWqzcvTsIyCZRga97jREIthGmZqq2kMpCF2oqBvaas10QuX17thnyxARwzTUrg/vICLBtW/Q3hLD19tjYynx7lS7WZKELNRUXT2vOtlgZylrl0Xuj7W3bgL173SpJ6rSQSlQHrIrpE1HVKyGfVR5JKklYdULkF6ti+oQpT9wrqsoj6cJrkkoS5rCJysHAXnO9i5UmtsXONAuvSSpJmMMmKgcDewOEi5XT08lmyGk60SadhRdxD+NG9qYhyoCBvUHCGXKrtfTcmjX219vSKlE15y6z8CIDbdS3DgZ86luqWvjjvPPOU8rH9LTqyIhqEOaCx8hI8HyvsbHlr+t92N7na/8+2I6h1Sp2HERFADCjDjGWM/aGSZJeiVt4TVNznvZGQ7bZddys21YRND/PGx5RH3OJ/r4fTZyxT08Hs0eR4M+yZoYi5hmsiPn14bhts3aR5a8ZGAj+tB1j0v2HYzDNricno2fd09P2/UUdD1FdwXHGzsDuQdHphyi2ID0wED2eJCmN3uDbfUJrtcyvHRtLN+aobdneJ5JuHERV5xrYmYrxoEr3ObalVxYXzaWMYapjdtbcPgBYeWyho0eBL3xh+cLlSy8BQ0MrtxNVu25bxF1cjH69LQ2jCuzZwxp66l8M7B6U0f7VlnsOq1YGBla+p/dk011RAgQBMQzuYbVLVCdHk4UFYN26ZLXrttp40zGEr2+37U3KxsZYQ099zmVa7/vRtFSMLSWQ19d+l9SPS67bZdxxlTM+8tjT06qDg/ZtmY4zKg3DyhdqKjAVU5w8L503zcxdUj+2WfD69Uvbs6Uyur9puLQs6BXV1tf2TcM2+zZ9i5iYsH8bUg0+B9asU19zif6+H02bsaumq4qJe49tZu4yUza91/XRai0fx+Ske/VJ1IzZdjy2hc6obz951OATVR1YFVNtpmDZG4zSVotE7cPlMTi4fBy2wGva9oUX2o85TVrHlt5xOXGxAoaaxjWwMxVTgnYbuOmmIPx0602n2FIli4tuqZ977125DxcLC0vjaLeDi31MVFemUL73PXsaJMticm96x6X5Ge9dSv2Kgb0EU1P2gNsdjGxVIQMDbhUfWQJb2C9m+3b7awYG4k9O3Wy591YrOo8/OBictHrz88DSnZqS7I+o6RjYSxAVcLuDka2Oe3HRrWuiS2CzLVqGN6COYhvf7Kx51m5bZN6zx16iCQCnnBL8aWv2xb7vRD1c8jW+H/2eY3ct1ctaRhmXh261zJftu+TlW63onLlt8XJycmmNYGAg+DkUVaIZ91lUpaUDUZ5QxOIpgPcDeALASQDjru/r98BuCrgiy4Oc7XVJqz3i+rxMTy9fHHWpUAlPQFH1570noN79mI4nKnin6UHj+tnwZEB1UVRgPwvAmQC+xcCejGtQyTP4ZCk/DN8/NBR9ArDtxzbrtp3IfF8EVqX+PkSuCgnsr2yEgb2Wohp/RaVj4ppwJX1db/296UTmOxAXfbUwkQ+ugb2wxVMR2SkiMyIyc/jw4aJ2SxFsi7jz80GYszlyJFi0jFoE7l68jKvOUV26CtW2KOy790sZ/X2IihIb2EXkQRF53PC4NMmOVPVmVR1X1fENGzakH3Gf83m7t7TlgPPzQUXK+vXm34flmEAwxqiTRMjlRto+75+a5KbcRHUTG9hV9V2qeo7hcXcRA6QlWe7vafp9mj4wobBXjanMcO/e4O/dnSNdt5ml1XGSkx5LJKnRXPI1cQ8wx16ItPf3jMpPd+e04xZMTbnx3vLFCy90y7275NuTSJODZ1UM1Q0Kqop5L4A5AL8G8HMAD7i8j4E9naTBNwxWrr1lkvZyibq7UlzgTrp4GReEuRhK/cA1sGdaPFXVu1R1VFVfpaqvU9WLsmyPlpjSCknzv7OzwJVXxt+JKJQkNRNesWq7u5JNeAy2fYULs92iUlAhLoYSdXGJ/r4fnLFHS3pzZ5e686jZbPdsuNVaKne0zfQB1bPPTv4Nojc14nLRkmq2G4Jwxk5NArbtra+odr29N48O8+RJUyLdOfao/HtU8F671n1/rZY5h+0SkF2uOuUFR9QPGNhrLCqY2oJV1NWZphOEy2X8qvHbimop4DJz9nULv+7PgIuh1FSugZ3dHSsoKpduKwkMa7yj+pMDS+WIYQ14XG46bnunnLL0GlunSGCpDXBvKaJLPbmtNHHbNnMbXx917kS15hL9fT/6ecbuMquMS61ElQTaGoyFM9wk1SS2HLhpLC5tA3p/npx0T6H0fm629QbO0qnJwFRM9STJAycpUzS91zUlkWShNmosUemjqHRLby29awqFi6XUjxjYKyhN7Xaa2WzSWavp/S75+u6x2Gb2q1a5nRiSHk8ebXyJqo6BvYLSBKO4IJdXNUjcDLx3LLbAHndDDpebVJuOhzN26kcM7BWURzDKK8Al3W5cusX2e9erX00VMMyxU79xDeysiilQHo2n8rri0rUSxaW6ZWICuOaalVUzpmN3PR7fbXyJmoSBvUB5BKO82s+axrpjR1AqmeaG0jfeCNx2W/yxs50ukQcu03rfj35NxeShyJREETeUTrJgzFQM9Rswx94/irriMmslis/7vHLxlPqRa2CX4LXFGh8f15mZmcL3S8m128GVrgcPBnl1U6fIsbHgSs+obezaFdx5qdvISPpU1KpVQSjvJRJceUrURCKyX1XH417HHDsZtdvAaacB27cv5dRNQT1u8Tdsudsb1IFsd0xiLp7IjoGdVogKxkBwT1PXxd+pqeie7Wmrd3hrOyI7BnZaIS4Ynzy5vNFW1L1G4wJ32hk2yx2J7FaXPQCqniTBOJzdhyeCsAQSCILs5s32G1pnnWFPTDCQE5lwxk4rRM2ie4OxaXbfnTu33QKv1Qpm2IB9tk9E6TCwV0RUOqNoccG4e5Ycd6WoKWUyPQ08/3zw+7h7mRJRcix3rIDedAaQrRTQ15jCMsfNm4NgbxrLli3mVEtcCWTW9xL1I5Y71khcOqMM4R2ZwkVSwPyNIkt1Sl59boj6HRdPK6DqAS5ugRRwm933si2sshadKBvO2Cug6hfbxH2j6J3du6aPWItOlA8G9gqoeoDL6xsFa9GJ8sHAXgFVD3B5fqNIO9snIjsG9oqocoCr+jcKIlouU2AXkc+KyFMi8kMRuUtETvU1MKqOqn+jIKLlMtWxi8h7AHxDVU+IyGcAQFU/Fvc+1rETESVXSB27qn5NVU90fnwIwGiW7RERUXY+c+xXA7jP4/aIiCiF2MAuIg+KyOOGx6Vdr5kCcAKAtcuHiOwUkRkRmTl8+LCf0VNqVepN46qOYyYqQ+ZeMSJyFYC/AXChqkZ08V7CHHu5qtibJk4dx0zkm2uOPevi6cUAPgfgD1TVeRrOwF6uOjbfquOYiXwrqgnYDQDWAfi6iDwmIjdl3B4VoOq9aUxsY5udZWqGqFemJmCqutXXQKg4dWy+FXUnpu5e7gBTM0S88rQP1fFKUtvNP7qV3eqYqCoY2PtQHa8k7R2zTZXTSURF4R2UqJa4mEr9iHdQokarYzqJqCgM7FRLdUwnERWFt8aj2pqYYCAnMuGMnYioYRjYiYgahoGdiKhhGNiJiBqGgZ2IqGEY2ImIGoaBnYioYRjYiYgahoGdiKhhGNiJiBqGgZ2IqGEY2ImIGoaBnYioYRjYiYgahoGdiKhhGNiJiBqGgZ2IqGEY2ImIGoaBnYioYRjYiYgahoGdiKhhGNiJiBomU2AXkX8UkR+KyGMi8jUReYOvgRERUTpZZ+yfVdW3qOpbAdwD4BMexkRERBlkCuyq+lLXj2sBaLbhEBFRVquzbkBEdgP4SwC/BPBHEa/bCWBn58dfi8jjWfddYacBeL7sQeSoycfX5GMDeHx1d6bLi0Q1epItIg8C+E3Dr6ZU9e6u1/0dgGFV/WTsTkVmVHXcZYB1xOOrryYfG8DjqzvX44udsavquxz32QZwL4DYwE5ERPnJWhXzxq4fLwXwVLbhEBFRVllz7J8WkTMBnAQwC+Aax/fdnHG/Vcfjq68mHxvA46s7p+OLzbETEVG98MpTIqKGYWAnImqY0gJ7k9sRiMhnReSpzvHdJSKnlj0mn0Tk/SLyhIicFJHGlJaJyMUi8rSIPCMiHy97PD6JyK0i8oumXj8iIptE5JsicqDzb3NX2WPyRUSGReRhEflB59g+FfuesnLsInJKeOWqiHwQwNmq6rr4Wmki8h4A31DVEyLyGQBQ1Y+VPCxvROQsBAvmXwTwEVWdKXlImYnIAIAfAXg3gDkAjwC4QlUPlDowT0TkAgBHAHxJVc8pezy+icjrAbxeVR8VkXUA9gP40yb89xMRAbBWVY+IyCCA7wLYpaoP2d5T2oy9ye0IVPVrqnqi8+NDAEbLHI9vqvqkqj5d9jg8Ox/AM6r6E1U9DuB2BCW8jaCq3wbwQtnjyIuqPqeqj3b+/jKAJwFsLHdUfmjgSOfHwc4jMl6WmmMXkd0i8iyACTS3gdjVAO4rexAUayOAZ7t+nkNDAkO/EZEtAM4F8P1yR+KPiAyIyGMAfgHg66oaeWy5BnYReVBEHjc8LgUAVZ1S1U0Irlq9Ps+x+BZ3bJ3XTAE4geD4asXl+IiqRqaY6JkAAAFLSURBVEReDeBOAB/qyQrUmqoudrrojgI4X0Qi02mZm4DFDKax7Qjijk1ErgLwJwAu1BpeLJDgv11THAKwqevn0c5zVBOd/POdANqq+pWyx5MHVX1RRL4J4GIA1oXwMqtiGtuOQEQuBvBRAJeo6tGyx0NOHgHwRhE5XUSGAFwO4Kslj4kcdRYYbwHwpKp+ruzx+CQiG8LKOhFZg2CBPzJellkVcyeCFpSvtCNQ1UbMkETkGQCvAjDfeeqhplT8AICIvBfAvwLYAOBFAI+p6kXljio7EdkG4PMABgDcqqq7Sx6SNyKyD8AfImhr+3MAn1TVW0odlEci8vsAvgPgfxHEFAD4e1W9t7xR+SEibwGwF8G/y1UA7lDVf4h8Tw2zBEREFIFXnhIRNQwDOxFRwzCwExE1DAM7EVHDMLATETUMAzsRUcMwsBMRNcz/A105Op3m+qmgAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"qF1skqMy2-K-"},"source":["Un perceptron simple (un seul neurone) comporte deux parties. Une partie linéaire, qui contient un produit scalaire et un \"biais\" ($b$) et une partie non linéaire, la fonction d'activation ($\\mathcal{A}$):\n","\\begin{equation*}\n"," f(x; \\omega,b) = \\mathcal{A}({\\sum} \\omega_i x_i  + b )  \\tag{1}\n","\\end{equation*}\n","\n","La classe P1 ci-dessous code pour des perceptrons définis sur $\\mathbb{R}^2$  et dont la fonction d'activation est une sigmoïde:\n","\\begin{equation*}\n","\\mathcal{A}(y) = \\dfrac{1}{1+e^{-y}}\n","\\end{equation*}\n","La fonction sigmoïde est à valeur dans [0,1]. On peut donc interpréter la sortie du neurone comme la probabilité d'appartenance à la première des deux classes. Dans la classe *P1*, le neurone renvoie d'ailleurs un vecteur de \"probabilités\":"]},{"cell_type":"code","metadata":{"id":"6z-ospVZ2-2Y"},"source":["class P1(nn.Module):\n","\n","    def __init__(self):\n","        super(P1, self).__init__()\n","        self.fc = nn.Linear(2,1)         \n","\n","    def forward(self, x):\n","      \n","        #produit scalaire et biais\n","        x = self.fc(x)\n","\n","        #activation                   \n","        x = x.sigmoid()\n","\n","        #vecteur de \"probabilités\" (cat: concaténation)\n","        x = torch.cat((x,1-x), dim = 1)  \n","        return x\n","\n","model = P1()\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9868s103BGa"},"source":["Dans la cellule suivante, on donne les commandes de base pour accéder aux poids d'une instance de *P1*."]},{"cell_type":"code","metadata":{"id":"44Rt28r48Mto","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666184664731,"user_tz":-120,"elapsed":13,"user":{"displayName":"pierre lepetit","userId":"00153244657746066434"}},"outputId":"aa0de637-520f-40d0-f525-9fed5b9f9ebd"},"source":["#initialisation (fantaisiste) des poids:\n","model.fc.weight[0,0].data.fill_(-0.1)\n","model.fc.weight[0,1].data.fill_(0.5)\n","model.fc.bias.data.fill_(-1)\n","\n","#récupération des poids:\n","fc = model.fc\n","weights = fc.weight.data.squeeze(dim=0)\n","bias = fc.bias.data\n","\n","print(weights)\n","print(bias)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.1000,  0.5000])\n","tensor([-1.])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z967YPGL9tv0"},"source":["**Exercice**: Vérifier sur un exemple l'égalité $f_c(x) = \\sum \\omega_i x_i  + b$ "]},{"cell_type":"markdown","metadata":{"id":"6ma4xPq23TRV"},"source":["Pour compléter la définition du perceptron, il faut une règle de décision. Cette règle est naturelle: pour $f(x ; \\omega,b) = (p_0, p_1)$, on choisit la classe $0$ ssi $p_0 > p_1$.  \\\\\n","\n","On peut tracer la frontière qui délimite les zones de décision du modèle. Donner son équation sous la forme $x_1 = \\alpha x_0 + \\beta$ où $\\alpha$ et $\\beta$ dépendent de $\\omega$ et $b$. \n","Compléter le code ci-dessous pour la tracer."]},{"cell_type":"code","metadata":{"id":"LiYkXSDg3SHP"},"source":["#Tracé de la droite séparatrice:\n","def traceFrontiere(weights, bias, ax, interval = [-10,10], color = 'black'):\n","  \n","    x0 = np.arange(interval[0],interval[1],0.01)\n","\n","    x1 = [...]\n","\n","    ax.plot(x0,x1,color = color)\n","    \n","    \n","traceFrontiere(weights.numpy(), bias.numpy(), ax)\n","\n","fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C0r0f6weRDzm"},"source":["Pour apprendre au perceptron à séparer correctement les classes, nous allons utiliser une descente de gradient stochastique par mini-batches. \n","Pour ça, nous avons besoin de pouvoir présenter au modèle des couples (*entrée*, *cible*) dans un ordre **aléatoire**.\n","Dans pytorch, cette sélection se fait grâce à deux objets: \n","- Une classe *Dataset*\n","- Une classe *Dataloader* \n"]},{"cell_type":"code","metadata":{"id":"ppUANOr4Udu8"},"source":["from torch.utils.data import Dataset, DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrxNimRfWBys"},"source":["Un *Dataset* pytorch contient une méthode d'accès aux données. La classe suivante donne un exemple rudimentaire. \\\\\n","Plus loin, nous intégrerons à nos datasets des étapes de chargement, de normalisation et d'augmentation de données. "]},{"cell_type":"code","metadata":{"id":"rL1j5_Ja3bGn"},"source":["class FirstDataset(Dataset):\n","    def __init__(self, inputs, targets):  \n","        self.inputs = inputs\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return self.targets.shape[0]\n","\n","    def __getitem__(self, idx):     #idx est un indice appelé par le loader\n","        x = self.inputs[idx,:]\n","        t = self.targets[idx]           \n","        return x, t                 #couple (input, target)\n","\n","inputs_train = torch.cat(echs, dim=0)\n","targets_train = torch.cat((torch.zeros(n),torch.ones(n)), dim = 0).long() \n","ds0 = FirstDataset(inputs_train, targets_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_uV0lyP62m84"},"source":["Un loader est un itérable python (comme les listes, les dictionnaires, etc) qu'on paramètre par la taille des batches et la méthode de sélection des données (avec ou sans remise, pondération, etc). En particulier, avec l'option shuffle = True, le tirage se fait sans remise, jusqu'à épuisement du jeu de données:\n"]},{"cell_type":"code","metadata":{"id":"3IVdc5dzZqnW"},"source":["loader = DataLoader(ds0,batch_size = 10, shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2rYh2U9kvg5"},"source":["Sur la figure suivante, on représente un premier batch de points tirés au hasard. A chaque exécution de la fenêtre qui suit, un nouveau batch de points est tiré jusqu'à épuisement."]},{"cell_type":"code","metadata":{"id":"J5anIQf2kxZ4"},"source":["fig2 = plt.figure()\n","ax2 = fig2.add_subplot(111)\n","ax2.axis([-3, 3, -3, 3])\n","\n","#premier batch de dix points\n","inputs, targets = next(iter(loader))\n","x,y = inputs.numpy()[:,0], inputs.numpy()[:,1]\n","cs = [colors[targets[i]] for i in range(len(targets))]\n","ax2.scatter(x,y, color = cs )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZ-wPRxqjnTf"},"source":["#Parcours du jeu de données\n","inputs, targets = next(iter(loader))\n","x,y = inputs.numpy()[:,0], inputs.numpy()[:,1]\n","cs = [colors[targets[i]] for i in range(len(targets))]\n","ax2.scatter(x,y, color = cs )\n","fig2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mW5rL2q9btNn"},"source":["Pour chaque batch mis à disposition, on calcule l'erreur modèle à l'aide d'une fonction de coût (loss function). Cette fonction de coût pénalise les écarts entre les sorties du réseau (ici $(p_0,p_1)$) et la cible (ici, une classe $c \\in \\{ 0 ; 1 \\}$).  \n","En classification on utilise en général la log vraisemblance (*Negative Log Likelihood*). Pour un point du batch, elle est définie par:\n","\n","$\\mathcal{L}((p_0,p_1), c)  = - ln(p_c)$   \n","\n","Cette quantité est moyennée sur chaque batch.\n"]},{"cell_type":"code","source":["def loss_fn(outputs, targets, show = False ):\n","\n","  # tous les -log(p)\n","  outputs = - torch.log(outputs)\n","  # tous les -log(pc)\n","  tensor_of_losses = torch.gather(outputs, 1, targets.unsqueeze(dim=1))\n","  # moyenne des -log(pc)\n","  loss = tensor_of_losses.mean()\n","\n","  if show:\n","    print(outputs)\n","    print(targets)\n","    print(tensor_of_losses)  \n","\n","  return loss"],"metadata":{"id":"DQAG6zJai-iH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCv5wvHl3i6J"},"source":["# Exemple : \n","inputs, targets = next(iter(loader))\n","l = loss_fn(model(inputs), targets, show = True)\n","\n","print(l)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iAGBKwo33rCg"},"source":["Pour chaque batch, on doit calculer les dérivées $\\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}}$ où $\\mathcal{L_{batch}}$ est la moyenne de la fonction de coût sur le batch. \\\\\n","\n","Pytorch garde en mémoire chacune des opérations effectuées avec les poids pour pouvoir appliquer les régles de dérivation usuelles. Ce calcul est lancé avec la méthode *.backward*. Les dérivées sont stockés avec les poids,\n","et sont accessibles avec *.grad*. "]},{"cell_type":"code","metadata":{"id":"FuEZyNv3qvi0"},"source":["w = model.fc.weight #[0,0]\n","print('avant backward:' + str(w.grad))\n","\n","l.backward()\n","\n","print('après backward:' + str(w.grad))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-IGQfo9-tbhB"},"source":["Dernière étape: la mise à jour des poids. \\\\\n","Pour ça, un grand nombre de méthodes sont disponibles. \n","On précise la méthode choisie à travers l'objet pytorch \"optimizer\". \\\\\n","La plus simple s'écrit: \\\\\n","\\begin{equation*}\n","w_i : = w_i - lr \\times \\dfrac{\\partial \\mathcal{L_{batch}}}{\\partial{\\omega_i}} \\tag{2} \n","\\end{equation*}\n","\n"," Le learning rate ($lr$) contrôle l'amplitude des incréments. "]},{"cell_type":"code","metadata":{"id":"riJaZIzL3u1I"},"source":["lr = 0.1\n","\n","#Deux méthodes de descente très utilisées:\n","optimizer = torch.optim.SGD(model.parameters(), lr = lr)            #correspond à l'équation (2)\n","#optimizer = torch.optim.Adam(model.parameters(), lr = lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L5C8lopm3D5q"},"source":["Dans la cellule suivante, on entraîne le perceptron. A chaque nouvelle éxécution, le jeu de données est parcouru deux fois (deux \"époques\")."]},{"cell_type":"code","metadata":{"id":"mUKXqbi93BJL"},"source":["for epoch in range(2):                   \n","    print(epoch)\n","\n","    #parcours aléatoire du jeu de données\n","    for x, label in loader:               \n","        \n","        #remise à zéro des gradients \n","        optimizer.zero_grad()             \n","        \n","        #calcul de (p0,p1)\n","        output = model(x)                 \n","        \n","        #calcul de l'erreur\n","        l = loss_fn(output, label)  \n","\n","        #\"calcul\" des gradients\n","        l.backward()                      \n","        \n","        #mise à jour des poids\n","        optimizer.step()                  \n","\n","    \n","    #Tracé de l'hyperplan\n","    fc = model.fc\n","    weights = fc.weight.data.squeeze(dim=0)\n","    bias = fc.bias.data\n","    traceFrontiere(weights.numpy(), bias.numpy(), ax)\n","\n","fig"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsuEDNaW6gZ7"},"source":["**Exercice**: compléter le code suivant pour tracer les gradients dans l'espace des poids $\\omega_0, \\omega_1$."]},{"cell_type":"code","metadata":{"id":"rQOGGGf_3xph"},"source":["#initialisation des poids:\n","model.fc.weight[0,0].data.fill_(-0.1)\n","model.fc.weight[0,1].data.fill_(0.5)\n","model.fc.bias.data.fill_(-1)\n","\n","fig3 = plt.figure()\n","ax3 = fig3.add_subplot(111)\n","ax3.axis([-1.5, 0, -1, 0.6])\n","\n","\n","loader = DataLoader(ds0,batch_size = 10,shuffle = True)\n","lr = 0.5\n","optimizer = torch.optim.SGD(model.parameters(), lr = lr)  \n","\n","for epoch in range(5):\n","    for x, label in loader:      \n","        optimizer.zero_grad()\n","        output = model(x)\n","        l = loss_fn(output, label)\n","        l.backward()\n","        \n","        #tracé des vecteurs:\n","        weights = ...\n","           \n","        ax3.scatter(...)        \n","        ax3.arrow(...)\n","        optimizer.step()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qriMTtoj2OQj"},"source":["**Exercice:** Déterminer la justesse (*accuracy*) du classifieur sur le set d'entraînement."]},{"cell_type":"code","metadata":{"id":"6HkNkf8X2VYX"},"source":["#%justesse sur le set d'entraînement:  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3lBC-seaXtM-"},"source":["**Remarque:** \n","Un modèle de la classe *P1* correspond à une classe de modèle statistique largment utilisé avec des prédicteurs de petites dimensions: la [régression logistique](http://wikistat.fr/pdf/st-m-modlin-reglog.pdf).\n","\n","Ce modèle est utilisé pour **expliquer et prédire** la valeur d'une variable qualitative binaire. \n","\n","Notons par exemple $Z$ une variable aléatoire à valeurs dans $\\{c_1, c_2\\}$. \n","La régression logistique par rapport au prédicteur $X = (X_1, X_2, ...)$ s'écrit :  \n","\n","$$ ℙ(Z = c_1 | X )  = \\sigma (\\sum \\omega_i X_i + b ) $$\n","\n","Où $\\sigma$ est la fonction sigmoïde. Dans le cadre d'une régression logistique, les poids $\\omega_i$ sont obtenus par **maximum de vraisemblance**."]},{"cell_type":"markdown","metadata":{"id":"_WnjGZCKG86n"},"source":["**B.** Maintenant, on se pose la question de la séparation d'ensembles de points plus complexes:"]},{"cell_type":"code","metadata":{"id":"PGBJ6qye30GD"},"source":["n = 100\n","std = 0.5\n","#échantillon 0:\n","meana = torch.tensor((-1.,-1.))\n","echa = meana + std*torch.randn(n,2) \n","meanb = torch.tensor((1.,1.))\n","echb = meanb + std*torch.randn(n,2) \n","\n","ech0 = torch.cat([echa,echb])\n","\n","\n","#échantillon 1:\n","meanc = torch.tensor((1.,-1.))\n","echc = meanc + std*torch.randn(n,2) \n","meand = torch.tensor((-1.,1.))\n","echd = meand + std*torch.randn(n,2) \n","\n","ech1 = torch.cat([echc,echd])\n","\n","echs= [ech0,ech1]\n","\n","#scatter plot:\n","\n","plt.figure(0)\n","plt.axis([-3, 3, -3, 3])\n","\n","plt.title('data')\n","colors = ['b','r']\n","labels = ['0','1']\n","\n","for i,ech in enumerate(echs):\n","    x,y = ech.numpy()[:,0], ech.numpy()[:,1]\n","    plt.scatter(x,y, color = colors[i] )\n","\n","plt.legend(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WHP4G-UHkXd"},"source":["**Exercice:** Quel est, approximativement, la meilleure justesse atteignable avec un modèle de la classe *P1* ? \n","Compléter la classe *P3* et entraîner un modèle pour obtenir une justesse d'au moins 90% sur le jeu d'entraînement. \\\\\n","Pourquoi le score au bout de 50 époques varie-t-il autant à chaque nouvel entraînement? \\\\\n","Est-il possible, avec une autre classe de modèle, d'obtenir une justesse de 100% ?"]},{"cell_type":"code","metadata":{"id":"WE3YEDzcIyQ9"},"source":["class P3(nn.Module):\n","\n","    def __init__(self):\n","        super(P3, self).__init__()\n","        self.fc1 = nn.Linear(2,2)     #première couche: 2 neurones\n","        self.fc2 = nn.Linear(2,1)     #deuxième couche: 1 neurone\n","\n","        \n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = self.fc1(x)\n","\n","        [...]\n","\n","        x = torch.cat((x,1-x), dim = 1)  #output de somme 1\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q9a4NufuOhKO"},"source":["**C.** Voyons maintenant comment généraliser l'approche à plusieurs classes:"]},{"cell_type":"code","metadata":{"id":"zz9rHdvYOfgE"},"source":["n = 100\n","std = 0.5\n","#échantillon 0:\n","mean0 = torch.tensor((-1.,-1.))\n","ech0 = mean0 + std*torch.randn(n,2) \n","\n","#échantillon 1:\n","mean1 = torch.tensor((1.,-1.))\n","ech1 = mean1 + std*torch.randn(n,2) \n","\n","#échantillon 2:\n","mean2 = torch.tensor((0.,1.))\n","ech2 = mean2 + std*torch.randn(n,2) \n","\n","echs= [ech0,ech1,ech2]\n","\n","\n","#scatter plot:\n","\n","plt.figure(0)\n","plt.axis([-3, 3, -3, 3])\n","\n","plt.title('data')\n","colors = ['b','r','g']\n","labels = ['0','1','2']\n","\n","for i,ech in enumerate(echs):\n","    x,y = ech.numpy()[:,0], ech.numpy()[:,1]\n","    plt.scatter(x,y, color = colors[i] )\n","\n","plt.legend(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z6-GFSNoQeFO"},"source":["Pour séparer ces points, un perceptron à deux couches devrait suffire. Le problème est de définir la fonction de coût. Pour continuer à utiliser la log vraisemblance, la couche de sortie va comporter autant de neurones que de classes. Pour définir une loi de probabilité, on utilise des exponentielles normalisées (fonction softmax):\n","\n","$p_i = \\dfrac{e^{y_i}}{\\sum{e^{y_j}}}$  \n","\n","Où les $y_i$ sont les sorties des neurones de la dernière couche."]},{"cell_type":"code","metadata":{"id":"bLJ4BA4cUOZf"},"source":["class P6(nn.Module):\n","\n","    def __init__(self):\n","        super(P6, self).__init__()\n","        self.fc1 = nn.Linear(2,2)     #première couche: 2 neurones\n","        self.fc2 = nn.Linear(2,3)     #deuxième couche: 3 neurones\n","\n","        \n","    def forward(self, x):\n","        # Max pooling over a (2, 2) window\n","        x = self.fc1(x)\n","        x = x.relu()\n","        x = self.fc2(x)\n","        x = x.softmax(dim=1)\n","        return x\n","\n","#torch.manual_seed(1)   #pour figer le générateur de nombres aléatoires\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHte1wxkU5EY"},"source":["**Exercice:** Créer les objets datasets et dataloader et vérifier qu'il est possible de séparer les points (justesse > 95 %)."]}]}