{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15zoRpYq7mUVumjEtkx3t7zLdwEOiL7jF","timestamp":1668761904628},{"file_id":"1CKMUrClhlVv1yY2x6kKSVhqq0WojrHgz","timestamp":1668673261907}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["TP n°2 : prédiction à l'échelle du pixel - cas faiblement supervisés"],"metadata":{"id":"MCN6FPP8tPo-"}},{"cell_type":"markdown","source":["Notions abordées:\n","\n","En partie I:\n","- Test d'un FCN-ResNet sur un problème de segmentation sémantique. Métriques adaptées.\n","- Appentissage parfaitement supervisé d'un U-Net sur un problème de débruitage. \n","\n","En partie II:\n","- Le scénario noise-to-noise\n","- Neural Eggs Separation (cas simplifié)\n","\n","Durée : 2 h"],"metadata":{"id":"_g6FpTC-rgwc"}},{"cell_type":"markdown","source":["## Partie II: débruitage d'images avec un FCN - cas faiblement supervisés\n","\n","\n"],"metadata":{"id":"EpYCZBnPH0RL"}},{"cell_type":"code","source":["#imports de base et montage du drive\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import time\n","\n","import torch\n","import torchvision\n","import torch.nn as nn   \n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, models, transforms\n","import torch.optim as optim\n","from PIL import Image"],"metadata":{"id":"iqp_Y4AhIqKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Monter le drive pour pouvoir importer **utile.py**. Changer le répertoire de travail à cet effet."],"metadata":{"id":"Wpu3GQiDAX0I"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShfsOSzse6-H","executionInfo":{"status":"ok","timestamp":1668774083628,"user_tz":-60,"elapsed":16012,"user":{"displayName":"pierre lepetit","userId":"00153244657746066434"}},"outputId":"47952460-75a4-4f13-8dd8-92d227c67ccb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#On se branche au répertoire contenant utile.py:\n","os.listdir()\n","os.chdir('drive/MyDrive/TP_ENM_2223/TP_2223_corr_P')"],"metadata":{"id":"rBewKJe7i4oa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.listdir() \n","from utile import *"],"metadata":{"id":"Ow4aFPufuCsq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Exercice 1: le scénario \"noise to noise\"\n","\n","Dans l'exercice 2 partie I, nous disposions de paires (version bruitée , version propre) sur lequelles apprendre.\n","Mais dans la réalité, on ne dispose pas toujours de versions propres.  \\\\\n","Par contre, il arrive que plusieurs versions bruitées soient disponibles. Penser par exemple à des photos prises avec un téléobjectif à plusieurs secondes d'écart: les effets de la turbulence de l'air sur la qualité de l'image sont indépendants d'une image à l'autre.  \\\\\n","Dans le scénario \"noise to noise\" [(Lehtinen,2018)](https://arxiv.org/pdf/1803.04189.pdf), on dispose de paires d'images bruitées indépendamment, rangées dans deux jeux $B^1_{noisy}$ et $B^2_{noisy}$.  \n","Pour nous placer dans ce scénario, nous reprenons les images de synthèse de l'exercice 1. La fonction *gen_noise2noise* permet d'échantillonner les deux bases:"],"metadata":{"id":"xypzPxQjBDzR"}},{"cell_type":"code","source":["image1, image2 = gen_noise2noise(6)\n","fig = plt.figure(0, figsize=(36, 6))\n","voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n","\n","fig2 = plt.figure(1, figsize=(36, 6))\n","voir_batch2D(image2.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1)"],"metadata":{"id":"2WAABDUPBDBr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q1** Définir une procédure d'apprentissage qui permette de débruiter convenablement l'image. On se laissera guider par l'intuition dans un premier temps."],"metadata":{"id":"55CM4Dt_r5o4"}},{"cell_type":"code","source":["fcn = UNet(1,1,16).cuda()  #1 canal entrée, 1 canal de sortie, paramètre taille du réseau: 16\n","\n","import torch.optim as optim\n","optimizer = optim.Adam(fcn.parameters(), 10**(-3))"],"metadata":{"id":"NvCsUxP9QF0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def criterion(output,target):\n","  return ...\n","\n","nepochs = 40 #réfléchir à la suite en attendant...\n","nbatches = 100\n","batchsize = 64\n","\n","train_losses = []\n","\n","\n","for epoch in range(nepochs):\n","    \n","\n","    print(\"Epoch \" + str(epoch))\n","    epoch_losses  = []\n","    for i in range(nbatches):  \n","        #Load inputs\n","        \n","        # Génération des images\n","\n","        # Passage sur GPU\n","\n","        # mise à zéro des gradients, forward pass\n","\n","        # coût et gradients\n","        \n","        # MAJ des poids\n","\n","        epoch_losses.append(loss.detach().cpu())\n","\n","        del target, input, loss\n","        torch.cuda.empty_cache()     \n","\n","    epoch_loss = np.mean(epoch_losses)\n","    train_losses.append(epoch_loss)    \n","    print('epoch loss : \\n')\n","    print(epoch_loss)"],"metadata":{"id":"57CKkJpoCm-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(train_losses)"],"metadata":{"id":"FHPsYB24RTq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualisation:\n","\n","fcn.eval()\n","\n","input, target = gen_noise2noise(6)\n","\n","output = fcn(input.cuda())\n","\n","\n","fig = plt.figure(0, figsize=(36, 6))\n","voir_batch2D(input, 6, fig, k=0, min_scale=0,max_scale=1)  #entrées\n","fig2 = plt.figure(1, figsize=(36, 6))\n","voir_batch2D(target, 6, fig2, k=0, min_scale=0,max_scale=1)  #cibles\n","fig3 = plt.figure(2, figsize=(36, 6))\n","voir_batch2D(output.detach().cpu(), 6, fig2, k=0, min_scale=0,max_scale=1) #sorties\n"],"metadata":{"id":"EIEkZ8U1RJo9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q2** analyse théorique:\n","\n","La procédure d'entraînement revient à chercher les poids $\\theta^*$ vérifiant:  \n","\\begin{equation}\n","\\theta^* = \\underset{\\theta}{argmin} {\\big \\{} \\mathbb{E}_{(X,Y)}[ \\ \\mathcal{L} (f_\\theta(X), Y) \\ ] {\\big \\}} \\tag{1}\n","\\end{equation}\n","\n","Réécrire l'équation (1) pour justifier la démarche suivie en **Q1**.\n","\n"],"metadata":{"id":"Qf3GTjMhsrQn"}},{"cell_type":"markdown","source":["On réécrit l'espérance à minimiser dans le cas d'une fonction de coût égale à la MAE:\n","\\begin{equation}\n"," \\mathbb{E}_{(X,Y)}[ \\ \\mathcal{L} (f_\\theta(X), Y) \\ ] = \\mathbb{E}_{X}[ \\mathbb{E}_{Y | X}\\ |f_\\theta(X) - Y | \\ ]\n","\\end{equation}\n","\n","Pour atteindre le minimum, le réseau doit fournir la médiane conditionnelle de $Y$ sachant $X$. Dans le cas d'une fonction de coût quadratique, le réseau proposera l'espérance conditionnelle. Contrairement à la médiane, celle-ci est biaisée, du fait que les éléments parasites sont toujours positifs.\n"],"metadata":{"id":"HUgkSArnkXeK"}},{"cell_type":"markdown","source":["###Exercice 2: le scénario \"NES\" (Neural Egg Separation)\n","Dans un autre scénario, deux sources d'images peuvent être échantillonnées indépendamment. La première fournit des images bruitées contenant l'objet d'intérêt. Les images provenant de la seconde source ne contiennent que du bruit. De plus, le processus de corruption, c'est à dire la façon dont le bruit est combiné à l'image propre, est supposé connu.\n","C'est un scénario relativement fréquent. Dans le cas d'images radar météorologique, par exemple, il est possible d'extraire des images ne contenant que du bruit en dehors des périodes de précipitations.  \n","Ce scénario a été défini par [Halperin et al (2018)](https://arxiv.org/pdf/1811.12739.pdf). Ces auteurs proposent une méthode itérative pour le traiter (Neural Egg Separation). Dans cet exercice, on s'appuiera sur un principe assez proche, toujours avec nos images de synthèse. \n","\n","Les fonctions *gen1_NES* et *gen2_NES* permettent d'échantillonner les deux sources. Le processus de corruption est trivial: le bruit (rectangles) est simplement sommé à l'image propre (cellules).   \n","\n"],"metadata":{"id":"qeHiaJE5O44_"}},{"cell_type":"code","source":["image1 = gen1_NES(6)\n","noise = gen2_NES(6)\n","\n","#échantilloner la source d'images bruitées:\n","fig = plt.figure(0, figsize=(36, 6))\n","voir_batch2D(image1, 6, fig, k=0, min_scale=0,max_scale=1)\n","\n","#échantilloner la source d'images ne contenant que du bruit:\n","fig2 = plt.figure(1, figsize=(36, 6))\n","voir_batch2D(noise, 6, fig2, k=0, min_scale=0,max_scale=1)"],"metadata":{"id":"NR-X2s_iQdW1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q1** Proposer une stratégie d'apprentissage qui débouche sur un débruitage efficace.\n","\n","**Q2** Discuter les limites de la méthode et évoquer quelques voies d'amélioration."],"metadata":{"id":"qn6-Nsd030qO"}}]}