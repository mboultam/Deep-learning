{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19Jm7DOjUNUKu7bELIgFmYsaMqbtLbmUL","timestamp":1642737360558}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EUCeXqO7icjq"},"source":["**Partie II** \n","\n","Dans cette partie, nous nous intéressons aux couches de convolution: ce sont les briques de base d'architectures classiques comme le VGG et le ResNet.\n","Pour comprendre leur effet, nous les manipulons un peu et nous introduisons les opérations de maxpooling qui leur sont souvent associées (**A.**). Puis nous observons comment ont convergé leurs paramètres après apprentissage sur la base de données ImageNet (**B.**). Nous visualisons aussi le signal en sortie des couches de convolution (**C.**)."]},{"cell_type":"code","metadata":{"id":"bkg70dgsiKte","executionInfo":{"status":"ok","timestamp":1668779095237,"user_tz":-60,"elapsed":4712,"user":{"displayName":"Salmane ELFTOUH","userId":"03068115658518632405"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","import torch\n","import torch.nn as nn   #couches prédéfinies\n","\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","#pour charger des modèles préentraînés sur ImageNet:\n","import torchvision\n","from torchvision import datasets, models, transforms"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_rUUdeimP-a"},"source":["Nous aurons aussi besoin d'accéder au dossier partagé. Pour cela, lancer drive.mount, cliquer sur le lien et entrer le mot de passe."]},{"cell_type":"markdown","metadata":{"id":"RFvBUEHpm5qt"},"source":["On vérifie qu'on a bien accès aux données:"]},{"cell_type":"code","metadata":{"id":"zUUklr_ijd2i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668779124641,"user_tz":-60,"elapsed":26574,"user":{"displayName":"Salmane ELFTOUH","userId":"03068115658518632405"}},"outputId":"dbf24600-3f44-4cc7-91d6-8e90245c04d6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"uDkaH-N8m4L9","executionInfo":{"status":"ok","timestamp":1668779129345,"user_tz":-60,"elapsed":263,"user":{"displayName":"Salmane ELFTOUH","userId":"03068115658518632405"}}},"source":["root = '/content/drive/TP_ENM/data'\n","#root = '/content/drive/Shareddrives/TP_ENM/data'\n","os.listdir(root)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXenRrQYlvh9"},"source":["**A.** Couches de convolution et maxpooling \\\\\n","\n","Avant de définir les couches de convolutions, éxaminons un peu deux modèles standards du deep learning (vgg16 et resnet50). Vous reconnaîtrez, dans les dernières couches, des perceptrons (\"classifier\"), formés à partir de la classe nn.Linear. \\\\\n","Mais, en première partie du réseau, vous voyez apparaître les 'conv2d' qui correspondent à ces fameuses convolutions.\n","\n"]},{"cell_type":"code","metadata":{"id":"WsvksdVgiow-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668779136382,"user_tz":-60,"elapsed":3647,"user":{"displayName":"Salmane ELFTOUH","userId":"03068115658518632405"}},"outputId":"f13b0fe7-2942-43bb-8e35-f0b80af60da5"},"source":["#Deux réseaux de neurones profonds: VGG16 et ResNet50\n","\n","model = models.vgg16(pretrained=False)\n","#model = models.resnet50(pretrained=False)\n","print(model)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"hgf-68FQibea"},"source":["Dans le cas le plus simple, en dimension 1, la \"convolution\" à un neurone est définie par une relation de la forme: \\\\\n","\\begin{equation*}\n","output_i = bias + \\sum_{j = 1}^n input_{i + j} \\times kernel_{j}  \\tag{1}\n","\\end{equation*}\n","Ici, $kernel$ représente un vecteur de taille $n$ qui contient les paramètres du neurone. Si, par exemple, $kernel$ est positif et de somme 1, c'est une moyenne mobile. Notons enfin que l'opérateur de convolution classique diffère légèrement ($input_{i - j}$ au lieu de  $input_{i + j}$). \\\\\n","\n","Le code pytorch est plus compliqué parce que:\n","- l'input est généralement un batch d'images comprenant plusieurs canaux\n","- il n'y a pas qu'un neurone\n","\n","La forme générale est définie [ici](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d).\n","\n","Sur cette page, $C_{in}$, $C_{out}$ et $N$  correspondent respectivement aux nombres de canaux d'entrée, de neurones de la couche, et à la taille du batch. \\\\\n","L'opérateur $\\star$ cache lui aussi des subtilités: il faut paramétrer la façon dont le noyau se déplace sur l'input (*stride*) et régler la question des bords (*padding*). Ces aspects, sur lesquels nous ne nous attarderons pas, sont plus faciles à comprendre à partir des animations de [Vincent Dumoulin](https://github.com/vdumoulin/conv_arithmetic). \\\\\n","\n","Pour illustrer l'effet d'une couche de convolution 2d à un neurone, chargeons une image RGB."]},{"cell_type":"code","metadata":{"id":"_jpo331Niqnx"},"source":["#%% Une image RGB:\n","from PIL import Image\n","path = os.path.join(root, 'cat.jpg')\n","\n","image = Image.open(path).convert(\"RGB\")\n","image = image.resize((256,256))\n","image  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1K-E1kVzuNY"},"source":["Les opérateurs de \"convolution\" utilisés par la classe Conv2d sont codés dans torch.nn.functional. Dans les lignes qui suivent, on définit un noyau gaussien, qu'on peut ensuite appliquer à l'image:"]},{"cell_type":"code","metadata":{"id":"TqsdGxCG0POy"},"source":["import torch.nn.functional as F  \n","\n","#définition d'un noyau gaussien de taille 10\n","x_range = torch.arange(-5, 5, 0.5)\n","y_range = torch.arange(-5, 5, 0.5)\n","xx, yy = torch.meshgrid(x_range, y_range)\n","var = 10\n","\n","kernel =  (1./(2.*3.14*var)) * torch.exp( - (xx**2 + yy**2)/(2*var) )  #gaussienne\n","\n","plt.imshow(kernel, cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y77_KgUL1YYX"},"source":["#On applique le même noyau à tous les canaux:\n","kernel = kernel.repeat(1,3,1,1)                        #on repète pour chaque canal R,G,B\n","\n","#on a ajouté au début la dimension associée à l'indexation de l'image dans le batch (\"batch dimension\"):\n","print(kernel.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uow4PpjvivTh"},"source":["#conversion de l'image cat.jpg en torch.tensor:\n","img = transforms.ToTensor()(image)\n","img = img.unsqueeze(dim=0)  #ajout de la \"batch dimension\"\n","\n","#convolution:\n","output = F.conv2d(img, kernel)\n","fig = plt.figure()\n","plt.imshow(output[0,0,:,:], cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6o0i9G94yDY"},"source":["En pratique, on utilise rarement des noyaux de taille aussi importante. Par exemple, dans un VGG, les noyaux sont de taille 3*3. Cela suffit à extraire \n","des caractéristiques intéressantes, comme les contours. \n","\n","**Exercice:** Appliquer un [filtre de Prewitt](https://fr.wikipedia.org/wiki/Filtre_de_Prewitt) à l'image à partir de la convolution2d pytorch (compléter le code suivant)."]},{"cell_type":"code","metadata":{"id":"ffdwzx9Y93bG"},"source":["#Gradient horizontal\n","kernel1 = torch.tensor([[-1.,0.,1.],[-1.,0.,1.],[-1.,0.,1.]])\n","kernel1 = kernel1.repeat(1,3,1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtpeaO5a4Mxg"},"source":["#Gradient vertical\n","kernel2 = \n","kernel2 = \n","\n","output1 = \n","output2 = \n","\n","output = (output1**2 + output2**2).sqrt()\n","\n","fig = plt.figure()\n","plt.imshow(output[0,0,:,:], cmap='gray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oezjOSOv72dQ"},"source":["Le filtre de Prewitt ne peut pourtant pas être encodé dans un réseau standard:\n","il fait intervenir un carré et une racine. Les non-linéarités qui sont implémentés dans ces réseaux sont plus simples:\n","\n","- la fonction ReLU. C'est simplement la fonction \"partie positive\".\n","- le maxpooling. c'est une forme de sous-échantillonnage. Dans sa forme la plus courante, on divise l'image en carrés de 2*2 pixels, et on renvoie la valeur maximum sur chaque carré. Les dimensions spatiales du tenseur de sortie sont donc divisées par deux."]},{"cell_type":"code","metadata":{"id":"OPDUoPbu4N6N"},"source":["#partie positive: fonction reLU\n","x = torch.rand(1,1,4,4) - 0.5\n","print(x)\n","print(x.relu())\n","\n","\n","#Maxpooling:\n","x = F.max_pool2d(x, kernel_size = 2)\n","print(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9C5qZXTR4O3K"},"source":["**Exercice:** Quelle est la taille du tenseur en sortie de la couche 30 du vgg16, comparée à celle de l'image d'entrée?"]},{"cell_type":"markdown","metadata":{"id":"W2Ya8zGxBUUC"},"source":[]},{"cell_type":"markdown","metadata":{"id":"H8vgTXpACyB6"},"source":["**B.** Les noyaux de convolution après apprentissage\n","\n","Observons maintenant les couches de convolution après un apprentissage sur un très gros jeu d'images annotées (~1 M) tirées de la base ImageNet ([http://www.image-net.org/challenges/LSVRC/2010/index](https://)). Dans un premier temps, voyons ce que deviennent les noyaux associés aux 64 neurones de la première couche de convolution d'un ResNet50 déjà entraîné:"]},{"cell_type":"code","metadata":{"id":"R558i9gSix-h"},"source":["#Avant apprentissage:\n","model = models.resnet50(pretrained=False)\n","first_layer = model.conv1.weight.data  \n","\n","print(first_layer.shape)\n","\n","plt.figure(figsize=(20, 17))\n","for i in range(first_layer.shape[0]):\n","    plt.subplot(8, 8, i+1) # \n","    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n","    plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZeDgGAYHXKE"},"source":["#Après apprentissage:\n","model = models.resnet50(pretrained=True)\n","first_layer = model.conv1.weight.data  \n","\n","print(first_layer.shape)\n","\n","plt.figure(figsize=(20, 17))\n","for i in range(first_layer.shape[0]):\n","    plt.subplot(8, 8, i+1) # \n","    plt.imshow(first_layer[i, 0, :, :], cmap='seismic')\n","    plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Blyqzr1nHq4P"},"source":["On retrouve, parmi ces noyaux, des extracteurs de contours similaires au filtre de Prewitt. \\\\\n","Les amateurs de traitement du signal reconnaîtront même des patrons très proches des [ondelettes de Morlet](https://www.google.com/search?q=wavelet+morlet+2d&rlz=1C1AVFC_enFR826FR857&hl=fr&sxsrf=ALeKk01sWHdzUO6bRogEv0KFx2gRgOWz_Q:1610077971021&source=lnms&tbm=isch&sa=X&ved=2ahUKEwiIq5fst4vuAhXPxYUKHaskDqAQ_AUoAXoECAUQAw&biw=915&bih=591  ). \\\\\n","Ce fait est remarquable: par une simple descente de gradient, on a fait émerger des filtres efficaces pour la reconnaissance des formes."]},{"cell_type":"markdown","source":["**Exercice:** Combien la première couche de convolution d'un ResNet50 contient-elle de \"neurones\" (un noyau par neurone)? \\\\\n","Combien y-a-il de poids dans un noyau ?\n","Combien cette couche contient-elle de poids ? "],"metadata":{"id":"blPVjMyTkOS3"}},{"cell_type":"code","source":[],"metadata":{"id":"-Z2vE2EykXEA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jaDoM_DKF7n"},"source":["**C.** Les cartes de caractéristiques\n","\n","Voyons maintenant ce que devient l'image à travers le réseau. Reprenons le VGG16, passons-le sur notre image de chat et voyons ce que devient le signal en se propageant dans le réseau. D'abord, voyons si le réseau reconnaît la bonne classe, parmi les mille classes du jeu de données. La liste des classes est [là](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).\n"]},{"cell_type":"code","source":["image = Image.open(path).convert(\"RGB\")\n","image = image.resize((256,256))\n","\n","img = transforms.ToTensor()(image)\n","\n","img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n","img = img.unsqueeze(dim=0)"],"metadata":{"id":"MtCqRvQTJ0TO"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpm97aPIi10Z"},"source":["model = models.vgg16(pretrained=True)\n","\n","#sortie brute\n","output = model(img)                        \n","\n","#fonction softmax\n","output = output.softmax(dim=1).detach()    \n","\n","#prédiction\n","_, c  =torch.max(output, dim=1)            \n","print(c)\n","\n","#\"probabilités\" associées aux classes\n","plt.plot(output.squeeze())  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCXk5gb1Px8Q"},"source":["Vous pourrez remarquer qu'en exécutant la dernière cellule plusieurs fois, les probabilités de sortie diffèrent alors que l'entrée ne change pas. Cela tient en fait au \"dropout\" (refaire un print(model)), qui désactive un sous-ensemble aléatoire des neurones du classifieur (cette opération permet de lutter contre le **surapprentissage**). \\\\\n","\n","Pour désactiver le \"dropout\" et figer le réseau, on passe en mode \"eval\" avec la commande: \\\\\n","\n","*model.eval()*"]},{"cell_type":"markdown","metadata":{"id":"bxPAUx_oNxeo"},"source":["Maintenant, visualisons le signal en sortie d'une couche de convolution. Pour cela, on peut utiliser la commande [*hook*](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html). \\\\\n","Les canaux de ces signaux intermédiaires sont appelés **cartes de caractéristiques** (*feature maps*). Voyons les cartes de caractéristiques associées à la première couche de convolution:"]},{"cell_type":"code","metadata":{"id":"ppPoNaz2i6V5"},"source":["z = []\n","#fonction qui permet de stocker dans z les cartes de caractéristiques \n","def store_layer_output(model, input, output):  \n","        z.append(output.detach())\n","\n","\n","model.features[0].register_forward_hook(store_layer_output)\n","model.features[10].register_forward_hook(store_layer_output)\n","model.features[17].register_forward_hook(store_layer_output)\n","model.features[28].register_forward_hook(store_layer_output)\n","\n","output = model(img)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UjiBuhjURxB"},"source":["for fm in z:\n","  print(fm.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjqiDDRTWWJq"},"source":["**Exercice:** Visualiser les cartes de caractéristiques des couches 0, 10, 17, 28. Vous remarquerez qu'au niveau 10, la réponse du neurone est souvent spécifique à un trait précis. "]},{"cell_type":"code","metadata":{"id":"CjzSPkJDPoaJ"},"source":["feature_maps = z[2]\n","\n","\n","[...]    \n","\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}