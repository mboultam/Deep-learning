{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MECih0q-q3cDJSxCAke82aKBmGW2Kwfw","timestamp":1672985393401}],"authorship_tag":"ABX9TyMFKpCdjlUo/Bs7MnDAc8bZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**TP n°6**\n","Notions abordées:\n","\n","En partie I:\n","- Prévision par RNN: un premier apprentissage sur une sinusoïde\n","- Apprentissage d'un LSTM sur une sinusoïde\n","\n","En partie II:\n","- Word embedding \n","- Apprentissage d'un LSTM plus complet\n","\n","En partie III:\n","- Reconnaissance auto. de la parole : utilisation d'un RNN-T\n","\n","En partie IV:\n","- Reconnaissance auto. de la parole : utilisation d'un Wav2Vec2\n","\n","\n","Durée : 3 h"],"metadata":{"id":"oJaMPbOSwKQs"}},{"cell_type":"markdown","source":["**Partie I**\n","\n","Cette partie introduit les RNN. Un RNN travaille sur une série temporelle de tenseurs. Il peut avoir pour objectif de prédire une classe pour la série, une classe pour chaque élément de la série, ou encore de prévoir le(s) prochain(s) éléments. \n","\n","La [page suivante](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) permet de visualiser différentes configurations pour l'apprentissage."],"metadata":{"id":"RQ_uIaydgAzs"}},{"cell_type":"markdown","source":["**Exercice 1** Dans un premier temps, nous allons entraîner un RNN en \"Many-to-One\" pour reconstruire une sinusoïde. \n","Instancier un RNN simple à l'aide du code qui suit."],"metadata":{"id":"WlLJZLMLi87u"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset\n","import torch\n","import matplotlib.pyplot as plt\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","from torch.utils.data import DataLoader\n","\n","class SimpleRNN(nn.Module):\n","\n","    def __init__(self, rnn_type, input_size, hidden_size, num_layers):\n","        super(SimpleRNN, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        \n","        if rnn_type == 'RNN':\n","            self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, dropout=(0 if num_layers == 1 else 0.05), num_layers=num_layers, batch_first=True)\n","        elif rnn_type == 'GRU':\n","            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, dropout=(0 if num_layers == 1 else 0.05), num_layers=num_layers, batch_first=True)\n","\n","        self.out = nn.Linear(hidden_size, 1)  \n","\n","    def forward(self, x, h_state):\n","        \n","        r_out, h_state = self.rnn(x, h_state)\n","\n","        final_y = self.out(r_out[:, -1, :])  \n","\n","        return final_y, h_state\n","    \n","RNN_TYPE = 'GRU'  \n","rnn = SimpleRNN(RNN_TYPE, input_size=1, hidden_size=4, num_layers=1).to(device)"],"metadata":{"id":"jNUa4FSpwbTU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q1** Que représentent les deux tenseurs *x* et *h_state* en entrée du RNN ?\n","Comment le mode \"Many-to-One\" se traduit-il ?"],"metadata":{"id":"dpW1lKn1kx7s"}},{"cell_type":"markdown","source":["**Q2** Que définissent les paramètres *hidden size* et *num_layers* ?"],"metadata":{"id":"aTAwBOGrkljf"}},{"cell_type":"markdown","source":["Donnons-nous maintenant des séquences sur lesquelles apprendre. Pour entraîner à reproduire une sinusoïde, nous allons extraire des sous-suites d'une série temporelle de base définie par $u_{t_i} = sin(t_i) $. Ces sous-suites sont mises à disposition grâce au Dataset suivant:"],"metadata":{"id":"4sJxJVPmlfd2"}},{"cell_type":"code","source":["class RNNDataset(Dataset):\n","\n","    def __init__(self, x, y=None):\n","        self.data = x\n","        self.labels = y\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        if self.labels is not None:\n","            return self.data[idx], self.labels[idx]\n","        else:\n","            return self.data[idx]\n","\n","\n","def create_dataset(sequence_length, train_percent=0.8):\n","\n","    # Create sin wave at discrete time steps.\n","    num_time_steps = 1500\n","    time_steps = np.linspace(start=0, stop=1000, num=num_time_steps, dtype=np.float32)\n","    discrete_sin_wave = (np.sin(time_steps * 0.5)).reshape(-1, 1)\n","\n","    # Take (sequence_length + 1) elements & put as a row in sequence_data, extra element is value we want to predict.\n","    # Move one time step and keep grabbing till we reach the end of our sampled sin wave.\n","    sequence_data = []\n","    for i in range(num_time_steps - sequence_length):\n","        sequence_data.append(discrete_sin_wave[i: i + sequence_length + 1, 0])\n","    sequence_data = np.array(sequence_data)\n","\n","    # Split for train/val.\n","    num_total_samples = sequence_data.shape[0]\n","    num_train_samples = int(train_percent * num_total_samples)\n","\n","    train_set = sequence_data[:num_train_samples, :]\n","    test_set = sequence_data[num_train_samples:, :]\n","\n","    print('{} total sequence samples, {} used for training'.format(num_total_samples, num_train_samples))\n","\n","    # Take off the last element of each row and this will be our target value to predict.\n","    x_train = train_set[:, :-1][:, :, np.newaxis]\n","    y_train = train_set[:, -1][:, np.newaxis]\n","    x_test = test_set[:, :-1][:, :, np.newaxis]\n","    y_test = test_set[:, -1][:, np.newaxis]\n","\n","    return x_train, y_train, x_test, y_test\n","\n"],"metadata":{"id":"C8Fu_StDmkI8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q3** Créer et tracer la série temporelle de base. "],"metadata":{"id":"SUQfm31imlgV"}},{"cell_type":"markdown","source":["**Q4** Instancier le Dataset, créer un Loader et le tester."],"metadata":{"id":"ezvyM0jenFi1"}},{"cell_type":"markdown","source":["**Q5** Que représente la cible $y$ par rapport à l'entrée $x$ ?"],"metadata":{"id":"GE8BpSGppbml"}},{"cell_type":"markdown","source":["**Q6** Compléter la boucle d'apprentissage suivante:"],"metadata":{"id":"1FSn-vBtnivk"}},{"cell_type":"code","source":["def train_model(model, dataloader, loss_function, optimizer, epochs):\n","    model.train()\n","    loss_all = []\n","\n","\n","    for epoch in range(epochs):\n","        for x_batch, y_batch in dataloader:\n","            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","            h_state = torch.zeros([model.num_layers, x_batch.size()[0], model.hidden_size]).to(device)\n","\n","            [...]\n","\n","        loss_all.append(loss.cpu().data.numpy())\n","        print('train loss epoch{}: '.format(epoch), loss.cpu().data.numpy())\n","\n","\n","\n","\n"],"metadata":{"id":"yNOQMuDKkhl1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q7** Lancer l'apprentissage avec des paramètres standards."],"metadata":{"id":"mT_cq2ZHoDC0"}},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","LEARNING_RATE = 0.01\n","BATCH_SIZE = 100\n","NUM_EPOCHS = 100\n","SEQUENCE_LENGTH = 50\n","\n","\n","# Define the model, optimizer and loss function.\n","\n","optimizer = torch.optim.Adam ...\n","loss_function = ...\n","\n","...\n"],"metadata":{"id":"963prazQoCRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q8** Le code suivant doit permettre de générer des prédictions avec le RNN.\n","Que fait-il ? Le tester.\n"],"metadata":{"id":"V5DF-OSSoQ0x"}},{"cell_type":"code","source":["def generate_predictions(model, dataloader, init_sequence_length):\n","    \"\"\"From a trained model predict \"\"\"\n","    model.eval()\n","\n","    h_state = torch.zeros([model.num_layers, 1, model.hidden_size]).to(device)  \n","    initial_input = next(iter(dataloader))[1].to(device) \n","    initial_input.unsqueeze_(0).data  \n","\n","    final_outputs = []\n","    for _ in range(len(dataloader.dataset.labels)-init_sequence_length):\n","\n","        output, _ = model(initial_input, h_state)\n","        final_outputs.append(output.cpu().squeeze_().data)\n","\n","        \n","        x = torch.clone(initial_input)\n","        initial_input.data[:, 0:init_sequence_length-1, :] = x.data[:, 1:init_sequence_length, :]\n","        initial_input.data[:, init_sequence_length-1, :] = output.data\n","\n","\n","    plt.plot(final_outputs, label='predicted')\n","    plt.plot(dataloader.dataset.labels[init_sequence_length:], label='actual')\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","    plt.show()\n","\n"],"metadata":{"id":"fUMYRI8hxDUh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Q9** Tester avec un RNN plus complexe (GRU) et interpréter."],"metadata":{"id":"P7DGzah1q5Sd"}},{"cell_type":"code","source":["generate_predictions(rnn, val_dataloader, init_sequence_length=SEQUENCE_LENGTH)"],"metadata":{"id":"kR4gqhvQxF45"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercice 2** Un LSTM sur une tâche de reconstruction de sinus.\n","\n","Le LSTM est un RNN prenant en entrée trois tenseurs: l'entrée courante, la sortie associée à l'entrée précédente et une \"mémoire à long terme\". Ce [post](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) résume clairement sont fonctionnement.\n","\n","Le code suivant doit permettre de reconstruire un sinus avec un LSTM. Préciser le mode sur lequel le LSTM est appris, compléter et commenter."],"metadata":{"id":"dfycjVYCqfme"}},{"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","#%matplotlib inline\n","\n","x = torch.linspace(0,799,800)\n","y = torch.sin(x*2*3.1416/40)\n","\n","plt.figure(figsize=(12,4))\n","plt.xlim(-10,801)\n","plt.grid(True)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"sin\")\n","plt.title(\"Sin plot\")\n","plt.plot(y.numpy(),color='#8000ff')\n","plt.show()"],"metadata":{"id":"sHwHsf8CsHyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_size = 40\n","train_set = y[:-test_size]\n","test_set = y[-test_size:]\n","\n","# voir l'ensemble de test\n","plt.figure(figsize=(12,4))\n","plt.xlim(-10,801)\n","plt.grid(True)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"sin\")\n","plt.title(\"Sin plot\")\n","plt.plot(train_set.numpy(),color='#8000ff')\n","plt.plot(range(760,800),test_set.numpy(),color=\"#ff8000\")\n","plt.show()"],"metadata":{"id":"Y77jAh_IsLBM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batches:\n","def input_data(seq,ws):\n","    out = []\n","    L = len(seq)\n","    \n","    for i in range(L-ws):\n","        window = seq[i:i+ws]\n","        label = seq[i+ws:i+ws+1]\n","        out.append((window,label))\n","    \n","    return out\n","\n","window_size = 40\n","train_data = input_data(train_set, window_size)\n","len(train_data)\n","\n","train_data[0]"],"metadata":{"id":"F9dWAjtHsQz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Le modèle :\n","class LSTM(nn.Module):\n","    \n","    def __init__(self,input_size = 1, hidden_size = 50, out_size = 1):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.lstm = nn.LSTM(input_size, hidden_size)\n","        self.linear = nn.Linear(hidden_size,out_size)\n","        self.hidden = (torch.zeros(1,1,hidden_size),torch.zeros(1,1,hidden_size))\n","    \n","    def forward(self,seq):\n","        lstm_out, self.hidden = self.lstm(seq.view(len(seq),1,-1), self.hidden)\n","        pred = self.linear(lstm_out.view(len(seq),-1))\n","        return ...\n","\n"],"metadata":{"id":"ygp4Bs2i0X5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# manual seed\n","torch.manual_seed(42)\n","model = LSTM()\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","\n","epochs = 20\n","future = 40\n","\n","#%%\n","for i in range(epochs):\n","    \n","    for seq, y_train in train_data:\n","        ...\n","        \n","    print(f\"Epoch {i} Loss: {loss.item()}\")\n","    \n","    preds = train_set[-window_size:].tolist()\n","\n","    # Test de la reconstruction:\n","    for f in range(future):\n","        seq = torch.FloatTensor(preds[-window_size:])\n","        with torch.no_grad():\n","            model.hidden = (torch.zeros(1,1,model.hidden_size),\n","                           torch.zeros(1,1,model.hidden_size))\n","            preds.append(model(seq).item())\n","        \n","    loss = criterion(torch.tensor(preds[-window_size:]), y[760:])\n","    print(f\"Performance on test range: {loss}\")\n","    \n","    plt.figure(figsize=(12,4))\n","    plt.xlim(700,801)\n","    plt.grid(True)\n","    plt.plot(y.numpy(),color='#8000ff')\n","    plt.plot(range(760,800),preds[window_size:],color='#ff8000')\n","    plt.show()\n"],"metadata":{"id":"RlH4ptSYsfxN"},"execution_count":null,"outputs":[]}]}